import nltk
import numpy as np
import random
import string
import bs4 as bs
import urllib.request
import re
import sklearn

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('wordnet')

# Web scraping
url = 'https://www.wikiwand.com/en/articles/Neuralink'

try:
    link = urllib.request.urlopen(url)  # Open URL
    print("Successfully opened the URL")
except Exception as e:
    print(f"Error opening URL: {e}")  # Print error if URL fails

data = bs.BeautifulSoup(link, 'html.parser')  # Default parser
    
if url:
    try:
        link = urllib.request.urlopen(url)
        link = link.read()
        data = bs.BeautifulSoup(link, 'lxml')
        data_paragraphs = data.find_all('p')
        data_text = ''

        for para in data_paragraphs:
            data_text += para.text    
        data_text = data_text.lower()
        data_text = re.sub(r'\[[0-9]*\]', ' ', data_text)
        data_text = re.sub(r'\s+', ' ', data_text)

        # Tokenization & Lemmatization
        sen = nltk.sent_tokenize(data_text)
        words = nltk.word_tokenize(data_text)
        wnlem = nltk.stem.WordNetLemmatizer()

        def perform_lemmatization(tokens):
            return [wnlem.lemmatize(token) for token in tokens]

        pr = dict((ord(punctuation), None) for punctuation in string.punctuation)

        def get_processed_text(document):
            return perform_lemmatization(nltk.word_tokenize(document.lower().translate(pr)))

        # Greetings
        greeting_inputs = ("hey", "hello", "whatsup", "hi")
        greeting_responses = ["hey", "hey hows you?", "*nods*", "hello, how you doing", "hello", "Welcome, I am good and you"]

        def generate_greeting_response(greeting):
            for token in greeting.split():
                if token.lower() in greeting_inputs:
                    return random.choice(greeting_responses)
            return None

        # Text similarity model
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        def generate_response(user_input):
            bot_response = ' '
            sen.append(user_input)

            # Preprocess sentences for TF-IDF
            preprocessed_sentences = [' '.join(get_processed_text(sentence)) for sentence in sen]

            word_vectorizer = TfidfVectorizer(stop_words='english')
            word_vectors = word_vectorizer.fit_transform(preprocessed_sentences)

            similar_vector_values = cosine_similarity(word_vectors[-1], word_vectors[:-1])
            similar_sentence_number = similar_vector_values.argsort()[0][-1]

            matched_vector = similar_vector_values.flatten()
            matched_vector.sort()
            vector_matched = matched_vector[-1]

            if vector_matched == 0:
                bot_response += "Sorry, I did not understand that. Please try again."
            else:
                bot_response += sen[similar_sentence_number]

            sen.pop(-1)  # Remove last input sentence
            return bot_response

        # Chatbot loop
        continue_flag = True
        print("Hello! Shoot your shot...")

        while continue_flag:
            human = input().lower().strip()
            if len(human) == 0:
                print("Bot: Please enter some text.")
                continue
            if human == 'bye':
                continue_flag = False
                print("Bot: Goodbye.")
                break
            elif human in ['thanks', 'thank you']:
                print("Bot: You're welcome! Goodbye.")
                continue_flag = False
                break
            else:
                greeting_response = generate_greeting_response(human)
                if greeting_response is not None:
                    print("Bot: " + greeting_response)
                else:
                    print("Bot: ", end="")
                    print(generate_response(human))
    except Exception as e:
        print(f"An error occurred: {e}")
else:
    print("The URL is empty. Please provide a valid URL.")
